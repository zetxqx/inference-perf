job:
  image:
    repository: quay.io/inference-perf/inference-perf
    tag: "main" # Defaults to .Chart.AppVersion

logLevel: DEBUG

hfToken: "hf_bicudPCAFSKbmVHyDVbEhvoaFugbhZZFy"

config:
  load:
    type: constant
    num_workers: 2
    worker_max_concurrency: 10
    stages:
    - rate: 20                      # Send all 20 users' requests per second
      duration: 5               
        
  api:
    type: completion
    streaming: true

  server:
    type: vllm
    model_name: Qwen/Qwen3-32B
    base_url: http://35.208.179.86
    ignore_eos: true

  tokenizer:
    pretrained_model_name_or_path: Qwen/Qwen3-32B

  data:
    type: shared_prefix
    shared_prefix:
      num_groups: 2                 # Number of distinct prefix, Note: the number of users is num_groups * num_prompts_per_group
      num_prompts_per_group: 10     # Number of unique questions per group (prefix)
      system_prompt_len: 100        # Length of the first prefix (in tokens), simulate initialization of a system prompt
      question_len: 50              # Length of the unique question part (in tokens)
      output_len: 50                # Target length for the model's generated output (in tokens)
      enable_multi_turn_chat: true
  
  metrics:
    type: prometheus
    prometheus:
      google_managed: false
      scrape_interval: 15

  report:
    request_lifecycle:
      summary: true
      per_stage: true
      per_request: false
    prometheus:
      summary: true
      per_stage: true

  storage:
    google_cloud_storage:
      bucket_name: "bobgkeinference" # IMPORTANT: Update with your GCS bucket name
      path: "quicktest3" # Optional: a path prefix within the bucket 