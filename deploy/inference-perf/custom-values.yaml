# custom-values.yaml
job:
  image: "quay.io/inference-perf/inference-perf:main"
  memory: "50G"

logLevel: DEBUG

hfToken: ""

config:
  # -- Server configuration
  server:
    type: vllm
    model_name: meta-llama/Llama-3.1-70B-Instruct
    base_url: http://35.208.246.59:80
    ignore_eos: true

  # -- Tokenizer configuration
  tokenizer:
    pretrained_model_name_or_path: meta-llama/Llama-3.1-70B-Instruct

  # -- Load configuration
  load:
    worker_max_concurrency: 2000
    type: constant
    interval: 10
    stages:
    - rate: 9           # warmup phase    
      duration: 25
    - rate: 2
      duration: 60
    - rate: 5
      duration: 60
    - rate: 8
      duration: 60
    - rate: 10
      duration: 60
    - rate: 15
      duration: 60
    - rate: 20
      duration: 60
    - rate: 25
      duration: 60
    - rate: 30
      duration: 60
    - rate: 40
      duration: 60

  # -- API configuration
  api:
    type: completion
    streaming: true

  # -- Data generation configuration
  data:
    type: shared_prefix
    shared_prefix:
      num_groups: 83              
      num_prompts_per_group: 3
      system_prompt_len: 8000
      question_len: 1000
      output_len: 1000

  # -- Metrics configuration for Prometheus
  # metrics:
  #   type: prometheus
  #   prometheus:
  #     url: http://your-prometheus-server-url:9090 # IMPORTANT: Update with your Prometheus URL
  #     scrape_interval: 15

  # -- Report configuration
  report:
    request_lifecycle:
      summary: true
      per_stage: true
      per_request: false
    prometheus:
      summary: true
      per_stage: false

  # -- Storage configuration for GCS
  storage:
    google_cloud_storage:
      bucket_name: "bobgkeinference" # IMPORTANT: Update with your GCS bucket name
      path: "precise-sho1" # Optional: a path prefix within the bucket