{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark vLLM Server with inference-perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local vLLM Setup using docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run vLLM Server as a docker container with the model HuggingFace `HuggingFaceTB/SmolLM2-135M-Instruct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71c1f998ef3488239cf88c97e0084e6287c87df3f3de3842e47c3751acc43329\n"
     ]
    }
   ],
   "source": [
    "!export MODEL_NAME=\"HuggingFaceTB/SmolLM2-135M-Instruct\" && \\\n",
    "    docker run --name vllm-server -d --runtime nvidia --gpus all \\\n",
    "    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n",
    "    -p 8000:8000 vllm/vllm-openai:latest \\\n",
    "    --model ${MODEL_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Configure [vLLM engine arguments](https://docs.vllm.ai/en/latest/serving/engine_args.html#engine-args) like `--max-model-len` and  `--max-num-seqs` according to local compute capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark with inference_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a configuration file for the test using `shareGPT` data and run the constant rate test for `30s`. You can also use any of the other data generators like `random`, `shared-prefix` or `synthetic` with their own configuration using the corresponding `config-*.yml` file instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "  type: shareGPT\n",
      "load:\n",
      "  type: constant\n",
      "  rate: 1\n",
      "  duration: 30\n",
      "vllm:\n",
      "  api: chat\n",
      "  model_name: HuggingFaceTB/SmolLM2-135M-Instruct\n",
      "  url: http://0.0.0.0:8000"
     ]
    }
   ],
   "source": [
    "!cat config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configuration from: config.yml\n",
      "Run started\n",
      "Run completed\n",
      "\n",
      "\n",
      "Generating Report ..\n",
      "total_requests: 38\n",
      "avg_prompt_tokens: 2.763157894736842\n",
      "avg_output_tokens: 28.94736842105263\n",
      "avg_time_per_request: 0.11538009351045873\n"
     ]
    }
   ],
   "source": [
    "!inference-perf --config_file config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete vLLM Server docker processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vllm-server\n",
      "vllm-server\n"
     ]
    }
   ],
   "source": [
    "!docker stop vllm-server && docker rm vllm-server"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
